{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the Results File: L2_intensities Obtained via Tierpsy Tracker"
      ],
      "metadata": {
        "id": "xkAR2YaEpXGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive connection"
      ],
      "metadata": {
        "id": "yZ3npAVDqDpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj7sniTEqFLP",
        "outputId": "473df4f3-a730-4577-ce86-0713081b3f06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "IEovfujWqODp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "N5tqloSwqPzA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting file content"
      ],
      "metadata": {
        "id": "-d6ou7_1qQUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_hdf5_datasets(file_path):\n",
        "    \"\"\"\n",
        "    Inspects an HDF5 file and prints the names and sizes of its datasets.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Inspecting file: {file_path}\")\n",
        "    print(f\"{'='*30}\")\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return  # Exit the function if the file doesn't exist\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdfid:\n",
        "            datasets_info = {}\n",
        "            for name, obj in hdfid.items():\n",
        "                if isinstance(obj, h5py.Dataset):\n",
        "                    datasets_info[name] = obj.size\n",
        "\n",
        "            if datasets_info:\n",
        "                print(\"Datasets found and their sizes:\")\n",
        "                for name, size in datasets_info.items():\n",
        "                    print(f\"  Dataset: {name}, Size: {size} elements\")\n",
        "            else:\n",
        "                print(\"No top-level datasets were found in this file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the file '{file_path}': {e}\")\n",
        "\n",
        "# Define the file path you want to analyze here.  REPLACE THIS!\n",
        "file_to_analyze = '/content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/L2_intensities.hdf5'  # <--- REPLACE THIS LINE\n",
        "\n",
        "inspect_hdf5_datasets(file_to_analyze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1r8KMr2qR-x",
        "outputId": "491a7c27-3b55-44fb-bba9-b1b40d95df75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Inspecting file: /content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/L2_intensities.hdf5\n",
            "==============================\n",
            "Datasets found and their sizes:\n",
            "  Dataset: straighten_worm_intensity_median, Size: 1349693 elements\n",
            "  Dataset: trajectories_data_valid, Size: 10303 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the datasets to individuals CSV files"
      ],
      "metadata": {
        "id": "7-xxGk53qbPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_hdf5_datasets_to_csv(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Exports all datasets from an HDF5 file to individual CSV files.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The path to the directory where the CSV files will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Exporting all datasets from: {file_path}\")\n",
        "    print(f\"CSV files will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            for dataset_name in hdf_file:\n",
        "                if isinstance(hdf_file[dataset_name], h5py.Dataset):\n",
        "                    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
        "                    dataset = hdf_file[dataset_name]\n",
        "                    data = dataset[:]  # Read all data\n",
        "\n",
        "                    # Convert to Pandas DataFrame\n",
        "                    df = pd.DataFrame(data)\n",
        "\n",
        "                    # Construct the CSV file path\n",
        "                    csv_file_path = os.path.join(output_dir, f\"{dataset_name}.csv\")\n",
        "\n",
        "                    # Export to CSV\n",
        "                    df.to_csv(csv_file_path, index=False)\n",
        "                    print(f\"Dataset '{dataset_name}' successfully exported to: {csv_file_path}\")\n",
        "                else:\n",
        "                    print(f\"Skipping: '{dataset_name}' is not a dataset.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nData export process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:  MODIFY THESE PATHS APPROPRIATELY\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/L2_intensities.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    export_hdf5_datasets_to_csv(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV2XoUbZqdRm",
        "outputId": "8d4bed3d-d9f2-4a05-f000-08647d9df2ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Exporting all datasets from: /content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/L2_intensities.hdf5\n",
            "CSV files will be saved in: /content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/Datasets\n",
            "==============================\n",
            "Skipping: 'provenance_tracking' is not a dataset.\n",
            "\n",
            "Processing dataset: straighten_worm_intensity_median\n",
            "Dataset 'straighten_worm_intensity_median' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/Datasets/straighten_worm_intensity_median.csv\n",
            "\n",
            "Processing dataset: trajectories_data_valid\n",
            "Dataset 'trajectories_data_valid' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/Datasets/trajectories_data_valid.csv\n",
            "\n",
            "Data export process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A problem arose because 'provenance_tracking' is not a dataset. It was  extracted separately:"
      ],
      "metadata": {
        "id": "5ktOKbPJqsUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note on non-dataset exports:\n",
        "#\n",
        "# The script exports standard HDF5 datasets to CSV files.  However, the following non-dataset item was handled specially:\n",
        "#\n",
        "# -  'provenance_tracking': This HDF5 group contains metadata attributes ('CLASS', 'TITLE', 'VERSION').\n",
        "#    These attributes were extracted and written to a single-row CSV file."
      ],
      "metadata": {
        "id": "cImIURKEq1Nd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_provenance_tracking(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Extracts the attributes from the 'provenance_tracking' group in an HDF5 file\n",
        "    and saves them to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The directory where the CSV file will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Extracting provenance_tracking from: {file_path}\")\n",
        "    print(f\"CSV file will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            if 'provenance_tracking' in hdf_file:\n",
        "                provenance_group = hdf_file['provenance_tracking']\n",
        "                attributes = {}\n",
        "                for attr_name, attr_value in provenance_group.attrs.items():\n",
        "                    attributes[attr_name] = attr_value\n",
        "                df = pd.DataFrame([attributes])  # Create a DataFrame with a single row\n",
        "                csv_file_path = os.path.join(output_dir, \"provenance_tracking.csv\")\n",
        "                df.to_csv(csv_file_path, index=False)\n",
        "                print(f\"Provenance tracking data successfully exported to: {csv_file_path}\")\n",
        "            else:\n",
        "                print(f\"Warning: 'provenance_tracking' group not found in HDF5 file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nProvenance tracking extraction process complete.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/L2_intensities.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/Groups'  # <--- Replace with the desired output folder\n",
        "\n",
        "    extract_provenance_tracking(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J6o2Bxkq-xc",
        "outputId": "a528a5c9-5087-4cfc-eb8a-2564d42700ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Extracting provenance_tracking from: /content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/L2_intensities.hdf5\n",
            "CSV file will be saved in: /content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/Groups\n",
            "==============================\n",
            "Provenance tracking data successfully exported to: /content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/Groups/provenance_tracking.csv\n",
            "\n",
            "Provenance tracking extraction process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of each dataset"
      ],
      "metadata": {
        "id": "K_BNR8CSC2Mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Top-Level Datasets"
      ],
      "metadata": {
        "id": "SmgmCl3iDJbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/Datasets'\n",
        "\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"Visualizing the first 5 rows of the datasets in: {csv_output_directory}\")\n",
        "print(f\"{'='*30}\")\n",
        "try:\n",
        "    # Check if the output directory exists\n",
        "    if not os.path.exists(csv_output_directory):\n",
        "        print(f\"Error: The directory '{csv_output_directory}' was not found.\")\n",
        "        exit()\n",
        "\n",
        "    # Iterate through all files in the specified directory\n",
        "    for filename in os.listdir(csv_output_directory):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(csv_output_directory, filename)\n",
        "            try:\n",
        "                # Read the CSV file into a Pandas DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                print(f\"\\n--- File: {filename} ---\")\n",
        "                print(\"First 5 rows:\")\n",
        "                if not df.empty:\n",
        "                    print(df.head())\n",
        "                else:\n",
        "                    print(\"The DataFrame is empty.\")\n",
        "\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: The file '{filename}' is empty.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading the file '{filename}': {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\nDataset visualization process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2gtMTX8C-AG",
        "outputId": "608fce83-9608-424a-891e-a2e0460c7a82"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Visualizing the first 5 rows of the datasets in: /content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/Datasets\n",
            "==============================\n",
            "\n",
            "--- File: straighten_worm_intensity_median.csv ---\n",
            "First 5 rows:\n",
            "       0       1       2       3       4       5       6       7       8  \\\n",
            "0  126.3  125.50  121.40  119.00  118.20  116.80  114.75  115.20  114.60   \n",
            "1  125.2  125.44  123.44  119.70  118.80  116.06  117.60  118.75  117.20   \n",
            "2  125.1  122.44  119.56  117.90  114.06  112.70  112.50  112.70  113.06   \n",
            "3  126.9  124.40  122.75  121.25  119.30  118.44  117.56  116.75  115.75   \n",
            "4  127.8  128.10  126.44  126.56  125.56  124.50  123.00  120.40  118.60   \n",
            "\n",
            "        9  ...     121     122     123     124     125     126     127  \\\n",
            "0  114.25  ...  105.30  106.06  107.70  109.90  112.50  115.56  119.30   \n",
            "1  116.60  ...  108.80  108.90  107.94  108.40  109.40  111.56  115.80   \n",
            "2  112.30  ...  111.40  112.40  113.25  113.94  114.40  116.00  118.56   \n",
            "3  113.60  ...  111.20  111.00  113.06  115.80  117.25  116.90  117.40   \n",
            "4  116.70  ...  110.44  112.25  115.06  117.56  119.06  120.40  123.60   \n",
            "\n",
            "      128     129     130  \n",
            "0  122.44  124.30  126.25  \n",
            "1  118.70  121.94  125.56  \n",
            "2  120.94  123.44  125.94  \n",
            "3  119.94  123.30  125.50  \n",
            "4  128.60  129.60  129.40  \n",
            "\n",
            "[5 rows x 131 columns]\n",
            "\n",
            "--- File: trajectories_data_valid.csv ---\n",
            "First 5 rows:\n",
            "   frame_number  worm_index_joined  plate_worm_id  skeleton_id    coord_x  \\\n",
            "0             0                  2             10           10  1042.8718   \n",
            "1             1                  2             11           11  1042.5466   \n",
            "2             2                  2             12           12  1042.3951   \n",
            "3             3                  2             13           13  1042.3761   \n",
            "4             4                  2             14           14  1042.4484   \n",
            "\n",
            "     coord_y  threshold  has_skeleton  roi_size   area  timestamp_raw  \\\n",
            "0  1547.4281      123.9             1      71.0  304.0            NaN   \n",
            "1  1545.5103      123.9             1      71.0  304.0            NaN   \n",
            "2  1543.8137      123.9             1      71.0  304.0            NaN   \n",
            "3  1542.3541      123.9             1      71.0  310.0            NaN   \n",
            "4  1541.1477      123.9             1      71.0  310.0            NaN   \n",
            "\n",
            "   timestamp_time  is_good_skel  skel_outliers_flag  int_map_id  \n",
            "0             NaN             1                   0           0  \n",
            "1             NaN             1                   0           1  \n",
            "2             NaN             1                   0           2  \n",
            "3             NaN             1                   0           3  \n",
            "4             NaN             1                   0           4  \n",
            "\n",
            "Dataset visualization process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Datasets extracted from the groups"
      ],
      "metadata": {
        "id": "GdEQ98LiDQnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/Groups'\n",
        "\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"Visualizing the first 5 rows of the datasets in: {csv_output_directory}\")\n",
        "print(f\"{'='*30}\")\n",
        "try:\n",
        "    # Check if the output directory exists\n",
        "    if not os.path.exists(csv_output_directory):\n",
        "        print(f\"Error: The directory '{csv_output_directory}' was not found.\")\n",
        "        exit()\n",
        "\n",
        "    # Iterate through all files in the specified directory\n",
        "    for filename in os.listdir(csv_output_directory):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(csv_output_directory, filename)\n",
        "            try:\n",
        "                # Read the CSV file into a Pandas DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                print(f\"\\n--- File: {filename} ---\")\n",
        "                print(\"First 5 rows:\")\n",
        "                if not df.empty:\n",
        "                    print(df.head())\n",
        "                else:\n",
        "                    print(\"The DataFrame is empty.\")\n",
        "\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: The file '{filename}' is empty.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading the file '{filename}': {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\nDataset visualization process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whiN1sZkDTLQ",
        "outputId": "73b7995e-b14e-41ce-f7b9-56fe06cfe66f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Visualizing the first 5 rows of the datasets in: /content/drive/MyDrive/Worms/Resultados/L2/L2_intensities/Groups\n",
            "==============================\n",
            "\n",
            "--- File: provenance_tracking.csv ---\n",
            "First 5 rows:\n",
            "      CLASS                     TITLE VERSION\n",
            "0  b'GROUP'  Empty(dtype=dtype('S1'))  b'1.0'\n",
            "\n",
            "Dataset visualization process completed.\n"
          ]
        }
      ]
    }
  ]
}